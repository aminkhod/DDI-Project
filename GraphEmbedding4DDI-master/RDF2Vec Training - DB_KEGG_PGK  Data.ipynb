{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gzip, os, csv\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirname = 'db_kegg_pgk_graph'\n",
    "entity2id_file= open(os.path.join(dirname, 'entity2id.txt'),'r')\n",
    "relation2id_file = open(os.path.join(dirname, 'relation2id.txt'),'r')\n",
    "entity2id ={}\n",
    "relation2id = {}\n",
    "for line in entity2id_file:\n",
    "    line  = line.strip().split()\n",
    "    if len(line) == 2:\n",
    "        entity2id[line[0]] = int(line[1]) \n",
    "\n",
    "for line in relation2id_file:\n",
    "    line  = line.strip().split()\n",
    "    if len(line) == 2:\n",
    "        relation2id[line[0]] = int(line[1]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, dirname, filename):\n",
    "        self.dirname = dirname\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for subfname in os.listdir(self.dirname):\n",
    "            if not self.filename in subfname: continue\n",
    "            fpath = os.path.join(self.dirname, subfname)\n",
    "            print ('Processing ',subfname)\n",
    "            for fname in os.listdir(fpath):\n",
    "                if not 'part' in fname: continue\n",
    "                if '.crc' in fname: continue\n",
    "                try:\n",
    "                    for line in open(os.path.join(fpath, fname), mode='r'):\n",
    "                        line = line.rstrip('\\n')\n",
    "                        words = line.split(\"->\")\n",
    "                        yield words\n",
    "                except Exception:\n",
    "                    print(\"Failed reading file:\")\n",
    "                    print(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractFeatureVector(model, drugs, id2entity, output): \n",
    "  \n",
    "    header=\"Drug\"\n",
    "    ns = \"n\"\n",
    "    first = ns+str(drugs[0])\n",
    "\n",
    "    for i in range(len(model.wv[first])):\n",
    "        header=header+\"\\tfeature\"+str(i)\n",
    "        \n",
    "    fw=open(output,'w')\n",
    "    fw.write(header+\"\\n\")\n",
    "\n",
    "    for id_ in sorted(drugs):\n",
    "        nid =ns+str(id_)\n",
    "        if  (nid) not in  model.wv:\n",
    "            print (nid)\n",
    "            continue\n",
    "        vec = model.wv[nid]\n",
    "        vec = \"\\t\".join(map(str,vec))\n",
    "        fw.write( id2entity[id_]+'\\t'+str(vec)+'\\n')\n",
    "    fw.close()\n",
    "    \n",
    "\n",
    "def extractFeatureVector_(model, entity2id, output): \n",
    "    drugsfilename = '../rdfvec/drubankids_ddi_v5.txt'\n",
    "    drugs = set()\n",
    " \n",
    "    drugsfile = open(drugsfilename)\n",
    "    #print (drugsfile.next())\n",
    "    for l in drugsfile:\n",
    "        #l=l.strip().replace('\"','').replace(\"http://bio2rdf.org/kegg:\",\"kegg:\").replace(\"http://bio2rdf.org/drugbank:\",\"db:\").replace(\"http://bio2rdf.org/pharmgkb:\",\"pharmgkb:\")\n",
    "        l= l.split()\n",
    "        drug = '<'+l[1]+'>'\n",
    "        if drug not in entity2id: continue\n",
    "        did  = 'n'+str(entity2id[drug])  \n",
    "        drugs.add((drug,did))\n",
    "\n",
    "    header=\"Drug\"\n",
    "    ns = \"n\"\n",
    "    first = list(drugs)[100]\n",
    "    first = '85141'\n",
    "\n",
    "    for i in range(len(model.wv[ns+first])):\n",
    "        header=header+\"\\tfeature\"+str(i)\n",
    "\n",
    "    fw=open(output,'w')\n",
    "    fw.write(header+\"\\n\")\n",
    "\n",
    "    for drug in sorted(drugs):\n",
    "        dbid=drug[0].replace('<http://bio2rdf.org/drugbank:','').replace('>','')\n",
    "        nid=drug[1]\n",
    "        if  (nid) not in  model.wv:\n",
    "            print (nid)\n",
    "            continue\n",
    "        vec = model.wv[nid]\n",
    "        vec = \"\\t\".join(map(str,vec))\n",
    "        fw.write( dbid+'\\t'+str(vec)+'\\n')\n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "\n",
    "\n",
    "def trainModel(drugs,id2entity, datafilename, model_output, vector_output, pattern, maxDepth):\n",
    "    \n",
    "    if not os.path.isdir(model_output):\n",
    "        os.mkdir(model_output)\n",
    "        \n",
    "    if not os.path.isdir(vector_output):\n",
    "        os.mkdir(vector_output)\n",
    "    \n",
    "    output = model_output + pattern +'/'\n",
    "    if not os.path.isdir(output):\n",
    "        os.mkdir(output)\n",
    "    \n",
    "    sentences = MySentences(datafilename, filename=pattern) # a memory-friendly iterator\n",
    "    model = gensim.models.Word2Vec(size=200, workers=5, window=5, sg=1, negative=15, iter=5)\n",
    "    #print sentences\n",
    "    model.build_vocab(sentences)\n",
    "    corpus_count = model.corpus_count\n",
    "    #sg/cbow features iterations window negative hops random walks\n",
    "    del model\n",
    "    model1 = gensim.models.Word2Vec(size=200, workers=8, window=5, sg=1, negative=15, iter=5)\n",
    "    model1.build_vocab(sentences)\n",
    "\n",
    "    #model1.train(sentences)\n",
    "    model1.train(sentences, total_examples=corpus_count, epochs =5)\n",
    "    modelname = 'Drug2Vec_sg_200_5_5_15_2_500'+'_d'+str(maxDepth)\n",
    "    model1.save(output+modelname)\n",
    "    \n",
    "    extractFeatureVector(model1, drugs, id2entity, vector_output+modelname+'_'+pattern+'.txt')\n",
    "    \n",
    "    #cbow 200\n",
    "    del model1\n",
    "    model2 = gensim.models.Word2Vec(size=200, workers=8, window=5, sg=0, iter=5,cbow_mean=1, alpha = 0.05)\n",
    "    model2.build_vocab(sentences)\n",
    "\n",
    "    model2.train(sentences, total_examples=corpus_count, epochs =5)\n",
    "    modelname = 'Drug2Vec_cbow_200_5_5_2_500'+'_d'+str(maxDepth)\n",
    "    model2.save(output+ modelname)\n",
    "    extractFeatureVector(model2, drugs, id2entity, vector_output+modelname+'_'+pattern+'.txt')\n",
    "    del model2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drugsfilename = 'data/input/drubankids_ddi_v5.txt'\n",
    "drugs = set()\n",
    "\n",
    "drugsfile = open(drugsfilename)\n",
    "for l in drugsfile:\n",
    "    l= l.split()\n",
    "    drug = '<'+l[1]+'>'\n",
    "    if drug not in entity2id: continue\n",
    "    did  = 'n'+str(entity2id[drug])  \n",
    "    drugs.add(did)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  randwalks_n250_depth4_uniform.txt\n",
      "Processing  randwalks_n250_depth1_uniform.txt\n",
      "Processing  randwalks_n250_depth3_uniform.txt\n",
      "Processing  randwalks_n250_depth2_uniform.txt\n",
      "Processing  randwalks_n250_depth4_uniform.txt\n",
      "Processing  randwalks_n250_depth1_uniform.txt\n",
      "Processing  randwalks_n250_depth3_uniform.txt\n",
      "Processing  randwalks_n250_depth2_uniform.txt\n",
      "Processing  randwalks_n250_depth4_uniform.txt\n"
     ]
    }
   ],
   "source": [
    "dataset = 'DB_KEGG_PGK/'\n",
    "datafilename = './walks/'+dataset\n",
    "model_output = './models/'+dataset    \n",
    "pattern = 'uniform'\n",
    "vector_output =  'vectors/'+dataset\n",
    "trainModel(drugs, entity2id, datafilename, model_output, vector_output, pattern, maxDepth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  randwalks_n250_depth3_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_split.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_split.txt\n"
     ]
    }
   ],
   "source": [
    "dataset = 'DB_KEGG_PGK/'\n",
    "datafilename = './walks/'+dataset\n",
    "model_output = './models/'+dataset    \n",
    "pattern = 'pagerank_split'\n",
    "vector_output =  'vectors/'+dataset\n",
    "trainModel(drugs, entity2id, datafilename, model_output, vector_output, pattern, maxDepth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  randwalks_n250_depth2_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth2_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth3_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth4_pagerank_pushdown.txt\n",
      "Processing  randwalks_n250_depth1_pagerank_pushdown.txt\n"
     ]
    }
   ],
   "source": [
    "dataset = 'DB_KEGG_PGK/'\n",
    "datafilename = './walks/'+dataset\n",
    "model_output = './models/'+dataset    \n",
    "pattern = 'pagerank_pushdown'\n",
    "vector_output =  'vectors/'+dataset\n",
    "trainModel(drugs, entity2id, datafilename, model_output, vector_output, pattern, maxDepth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
