{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gzip, os, csv\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[10] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "if False: \n",
    "    sc.stop()\n",
    "\n",
    "config = SparkConf()\n",
    "config.setMaster(\"local[10]\")\n",
    "config.set(\"spark.executor.memory\", \"70g\")\n",
    "config.set('spark.driver.memory', '90g')\n",
    "config.set(\"spark.memory.offHeap.enabled\",True)\n",
    "config.set(\"spark.memory.offHeap.size\",\"50g\") \n",
    "sc = SparkContext(conf=config)\n",
    "print (sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addTriple(net, source, target, edge):\n",
    "    if source in net:\n",
    "        if  target in net[source]:\n",
    "            net[source][target].add(edge)\n",
    "        else:\n",
    "            net[source][target]= set([edge])\n",
    "    else:\n",
    "        net[source]={}\n",
    "        net[source][target] =set([edge])\n",
    "            \n",
    "def getLinks(net, source):\n",
    "    if source not in net:\n",
    "        return {}\n",
    "    return net[source]\n",
    "\n",
    "def randomWalkUniform(triples, startNode, max_depth=5):\n",
    "    next_node =startNode\n",
    "    path = 'n'+str(startNode)+'->'\n",
    "    for i in range(max_depth):\n",
    "        neighs = getLinks(triples,next_node)\n",
    "        #print (neighs)\n",
    "        if len(neighs) == 0: break\n",
    "        weights = []\n",
    "        queue = []\n",
    "        for neigh in neighs:\n",
    "            for edge in neighs[neigh]:\n",
    "                queue.append((edge,neigh))\n",
    "                weights.append(1.0)\n",
    "        edge, next_node = random.choice(queue)\n",
    "        path = path+ 'e'+str(edge)+'->'\n",
    "        path = path+ 'n'+str(next_node)+'->'\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(folders, filename):\n",
    "    entity2id = {}\n",
    "    relation2id = {}\n",
    "    triples = {}\n",
    "\n",
    "    ent_counter = 0\n",
    "    rel_counter = 0\n",
    "    for dirname in folders:\n",
    "        for fname in os.listdir(dirname):\n",
    "            if not filename in fname: continue\n",
    "            print (fname)\n",
    "            gzfile= gzip.open(os.path.join(dirname, fname), mode='rt')\n",
    "\n",
    "            for line in csv.reader(gzfile, delimiter=' ', quotechar='\"'):\n",
    "                h = line[0]\n",
    "                r = line[1]\n",
    "                t = line[2]\n",
    "\n",
    "                if not t.startswith('<'): continue\n",
    "                if 'ddi-interactor-in' in r: \n",
    "                     continue\n",
    "\n",
    "                if h in entity2id:\n",
    "                    hid = entity2id[h]\n",
    "                else:\n",
    "                    entity2id[h] = ent_counter\n",
    "                    ent_counter+=1\n",
    "                    hid = entity2id[h]\n",
    "\n",
    "                if t in entity2id:\n",
    "                    tid = entity2id[t]\n",
    "                else:\n",
    "                    entity2id[t] = ent_counter\n",
    "                    ent_counter+=1\n",
    "                    tid = entity2id[t]\n",
    "\n",
    "                if r in relation2id:\n",
    "                    rid = relation2id[r]\n",
    "                else:\n",
    "                    relation2id[r] = rel_counter\n",
    "                    rel_counter+=1\n",
    "                    rid = relation2id[r]\n",
    "                addTriple(triples, hid, tid, rid)\n",
    "            print ('Relation:',rel_counter, ' Entity:',ent_counter)\n",
    "    return entity2id,relation2id,triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drugbank_old.nq.gz\n",
      "Relation: 76  Entity: 574152\n"
     ]
    }
   ],
   "source": [
    "folders = ['./rdf_data/drugbank']\n",
    "fileext = 'nq.gz'\n",
    "entity2id, relation2id, triples = preprocess(folders, fileext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of triples 2588933\n"
     ]
    }
   ],
   "source": [
    "num_triples=0\n",
    "for source in triples:\n",
    "    for  target in triples[source]:\n",
    "        num_triples+=len(triples[source][target])\n",
    "print ('Number of triples',num_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomNWalkUniform(triples, n, walks, path_depth):\n",
    "    path=[]\n",
    "    for k in range(walks):\n",
    "        walk = randomWalkUniform(triples, n, path_depth)\n",
    "        path.append(walk)\n",
    "    path = list(set(path))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n100->e0->n13->\n"
     ]
    }
   ],
   "source": [
    "walks = 5\n",
    "path_depth = 10\n",
    "paths = randomNWalkUniform(triples, 100, walks, path_depth)\n",
    "print('\\n'.join(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entities = list(entity2id.values())\n",
    "b_triples = sc.broadcast(triples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./walks/randwalks_n250_depth1_pagerank_uniform.txt\n",
      "Time elapsed to generate features: 00:01:36\n",
      "./walks/randwalks_n250_depth2_pagerank_uniform.txt\n",
      "Time elapsed to generate features: 00:02:05\n",
      "./walks/randwalks_n250_depth3_pagerank_uniform.txt\n",
      "Time elapsed to generate features: 00:02:23\n",
      "./walks/randwalks_n250_depth4_pagerank_uniform.txt\n",
      "Time elapsed to generate features: 00:02:35\n"
     ]
    }
   ],
   "source": [
    "folder = './walks/'\n",
    "if not os.path.isdir(folder):\n",
    "    os.mkdir(folder)\n",
    "walks = 250\n",
    "maxDepth = 5\n",
    "for path_depth in range(1,maxDepth):\n",
    "    filename = folder+'randwalks_n%d_depth%d_pagerank_uniform.txt'%(walks, path_depth)\n",
    "    print (filename)\n",
    "    start_time =time.time()\n",
    "    rdd = sc.parallelize(entities).flatMap(lambda n: randomNWalkUniform(b_triples.value, n, walks, path_depth))\n",
    "    rdd.saveAsTextFile(filename)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print ('Time elapsed to generate features:',time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveData(entity2id, relation2id, triples, dirname):\n",
    "    if not os.path.isdir(dirname):\n",
    "        os.mkdir(dirname)  \n",
    "    \n",
    "    entity2id_file= open(os.path.join(dirname, 'entity2id.txt'),'w')\n",
    "    relation2id_file = open(os.path.join(dirname, 'relation2id.txt'),'w')\n",
    "    train_file = open(os.path.join(dirname, 'train2id.txt'),'w')\n",
    "\n",
    "    train_file.write(str(num_triples)+'\\n') \n",
    "    for source in triples:\n",
    "        for  target in triples[source]:  \n",
    "            hid=source\n",
    "            tid =target\n",
    "            for rid  in triples[source][target]:\n",
    "                train_file.write(\"%d %d %d\\n\"%(hid,tid,rid))\n",
    "\n",
    "    entity2id_file.write(str(len(entity2id))+'\\n')  \n",
    "    for e in sorted(entity2id, key=entity2id.__getitem__):\n",
    "        entity2id_file.write(e+'\\t'+str(entity2id[e])+'\\n')  \n",
    "\n",
    "    relation2id_file.write(str(len(relation2id))+'\\n')    \n",
    "    for r in sorted(relation2id, key=relation2id.__getitem__):\n",
    "        relation2id_file.write(r+'\\t'+str(relation2id[r])+'\\n') \n",
    "        \n",
    "    train_file.close()\n",
    "    entity2id_file.close()\n",
    "    relation2id_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'db_graph'\n",
    "saveData(entity2id, relation2id, triples, dirname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname, filename):\n",
    "        self.dirname = dirname\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        print ('Processing ',self.filename)\n",
    "        for subfname in os.listdir(self.dirname):\n",
    "            if not self.filename in subfname: continue\n",
    "            fpath = os.path.join(self.dirname, subfname)\n",
    "            for fname in os.listdir(fpath):\n",
    "                if not 'part' in fname: continue\n",
    "                if '.crc' in fname: continue\n",
    "                try:\n",
    "                    for line in open(os.path.join(fpath, fname), mode='r'):\n",
    "                        line = line.rstrip('\\n')\n",
    "                        words = line.split(\"->\")\n",
    "                        yield words\n",
    "                except Exception:\n",
    "                    print(\"Failed reading file:\")\n",
    "                    print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractFeatureVector(model, drugs, id2entity, output): \n",
    "  \n",
    "    header=\"Entity\"\n",
    "    ns = \"n\"\n",
    "    first = ns+str(drugs[0])\n",
    "\n",
    "    for i in range(len(model.wv[first])):\n",
    "        header=header+\"\\tfeature\"+str(i)\n",
    "        \n",
    "    fw=open(output,'w')\n",
    "    fw.write(header+\"\\n\")\n",
    "\n",
    "    for id_ in sorted(drugs):\n",
    "        nid =ns+str(id_)\n",
    "        if  (nid) not in  model.wv:\n",
    "            print (nid)\n",
    "            continue\n",
    "        vec = model.wv[nid]\n",
    "        vec = \"\\t\".join(map(str,vec))\n",
    "        fw.write( id2entity[id_]+'\\t'+str(vec)+'\\n')\n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def trainModel(drugs, id2entity, datafilename, model_output, vector_output, pattern, maxDepth):\n",
    "    \n",
    "    if not os.path.isdir(model_output):\n",
    "        os.mkdir(model_output)\n",
    "        \n",
    "    if not os.path.isdir(vector_output):\n",
    "        os.mkdir(vector_output)\n",
    "    \n",
    "    output = model_output + pattern +'/'\n",
    "    if not os.path.isdir(output):\n",
    "        os.mkdir(output)\n",
    "    \n",
    "    sentences = MySentences(datafilename, filename=pattern) # a memory-friendly iterator\n",
    "    model = gensim.models.Word2Vec(size=200, workers=5, window=5, sg=1, negative=15, iter=5)\n",
    "\n",
    "    model.build_vocab(sentences)\n",
    "    corpus_count = model.corpus_count\n",
    "    #sg/cbow features iterations window negative hops random walks\n",
    "    del model\n",
    "    model1 = gensim.models.Word2Vec(size=200, workers=8, window=5, sg=1, negative=15, iter=5)\n",
    "    model1.build_vocab(sentences)\n",
    "\n",
    "    model1.train(sentences, total_examples=corpus_count, epochs =5)\n",
    "    modelname = 'Entity2Vec_sg_200_5_5_15_2_500'+'_d'+str(maxDepth)\n",
    "    model1.save(output+modelname)\n",
    "    \n",
    "    extractFeatureVector(model1, drugs, id2entity, vector_output+modelname+'_'+pattern+'.txt')\n",
    "    \n",
    "    #cbow 200\n",
    "    del model1\n",
    "    model2 = gensim.models.Word2Vec(size=200, workers=8, window=5, sg=0, iter=5,cbow_mean=1, alpha = 0.05)\n",
    "    model2.build_vocab(sentences)\n",
    "\n",
    "    model2.train(sentences, total_examples=corpus_count, epochs =5)\n",
    "    modelname = 'Entity2Vec_cbow_200_5_5_2_500'+'_d'+str(maxDepth)\n",
    "    model2.save(output+ modelname)\n",
    "    extractFeatureVector(model2, drugs, id2entity, vector_output+modelname+'_'+pattern+'.txt')\n",
    "    del model2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ddi_df = pd.read_csv('data/input/ddi_v5.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_ns ='http://bio2rdf.org/drugbank:'\n",
    "ddi_df.Drug1 = '<'+db_ns+ddi_df.Drug1+'>'\n",
    "ddi_df.Drug2 = '<'+db_ns+ddi_df.Drug2+'>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug1</th>\n",
       "      <th>Drug2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://bio2rdf.org/drugbank:DB00001&gt;</td>\n",
       "      <td>&lt;http://bio2rdf.org/drugbank:DB01048&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://bio2rdf.org/drugbank:DB00001&gt;</td>\n",
       "      <td>&lt;http://bio2rdf.org/drugbank:DB00054&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://bio2rdf.org/drugbank:DB00001&gt;</td>\n",
       "      <td>&lt;http://bio2rdf.org/drugbank:DB06736&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://bio2rdf.org/drugbank:DB00001&gt;</td>\n",
       "      <td>&lt;http://bio2rdf.org/drugbank:DB01418&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://bio2rdf.org/drugbank:DB00001&gt;</td>\n",
       "      <td>&lt;http://bio2rdf.org/drugbank:DB00945&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Drug1  \\\n",
       "0  <http://bio2rdf.org/drugbank:DB00001>   \n",
       "1  <http://bio2rdf.org/drugbank:DB00001>   \n",
       "2  <http://bio2rdf.org/drugbank:DB00001>   \n",
       "3  <http://bio2rdf.org/drugbank:DB00001>   \n",
       "4  <http://bio2rdf.org/drugbank:DB00001>   \n",
       "\n",
       "                                   Drug2  \n",
       "0  <http://bio2rdf.org/drugbank:DB01048>  \n",
       "1  <http://bio2rdf.org/drugbank:DB00054>  \n",
       "2  <http://bio2rdf.org/drugbank:DB06736>  \n",
       "3  <http://bio2rdf.org/drugbank:DB01418>  \n",
       "4  <http://bio2rdf.org/drugbank:DB00945>  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_entities = set()\n",
    "drugs = set(ddi_df.Drug1.unique()).union(ddi_df.Drug2.unique())\n",
    "for dbid in drugs:\n",
    "    if dbid in entity2id:\n",
    "        db_entities.add(entity2id[dbid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2124\n"
     ]
    }
   ],
   "source": [
    "db_entities =list(db_entities)\n",
    "print (len(db_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2entity = { value:key for key,value in entity2id.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "datafilename = './walks/'\n",
    "model_output = './models/'    \n",
    "pattern = 'uniform'\n",
    "vector_output =  './vectors/'\n",
    "trainModel(db_entities, id2entity, datafilename, model_output, vector_output, pattern, maxDepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
